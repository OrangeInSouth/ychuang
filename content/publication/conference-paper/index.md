---
abstract: Although all-in-one-model multilingual neural machine translation
  (MNMT) has achieved remarkable progress, the convergence inconsistency in the
  joint training is ignored, i.e.,different language pairs reaching convergence
  in different epochs. This leads to the trained MNMT model over-fitting
  low-resource language translations while under-fitting high-resource ones. In
  this paper, we propose a novel training strategy named LSSD (LanguageSpecific
  Self-Distillation), which can alleviate the convergence inconsistency and help
  MNMT models achieve the best performance on each language pair simultaneously.
  Specifically, LSSD picks up language-specific best checkpoints for each
  language pair to teach the current model on the fly. Furthermore, we
  systematically explore three sample-level manipulations of knowledge
  transferring. Experimental results on three datasets show that LSSD obtains
  consistent improvements towards all language pairs and achieves the
  state-of-the-art.
slides: ""
url_pdf: ""
publication_types:
  - "1"
authors:
  - "**Yichong Huang**"
  - Xinwei Geng
  - Xiaocheng Feng
  - Bing Qin
author_notes: []
publication: In *EMNLP2022*
summary: ""
url_dataset: https://github.com/wowchemy/wowchemy-hugo-themes
url_project: ""
publication_short: ""
url_source: https://github.com/wowchemy/wowchemy-hugo-themes
url_video: https://youtube.com
title: Unifying the Convergences in Multilingual Neural Machine Translation
doi: ""
featured: false
tags: []
projects: []
image:
  caption: ""
  focal_point: ""
  preview_only: false
date: 2022-11-28T13:59:00.000Z
url_slides: ""
publishDate: 2017-01-01T00:00:00.000Z
url_poster: ""
url_code: https://github.com/wowchemy/wowchemy-hugo-themes
---
